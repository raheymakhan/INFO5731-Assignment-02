{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USSdXHuqnwv9"
   },
   "source": [
    "# **INFO5731 Assignment Two**\n",
    "\n",
    "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWxodXh5n4xF"
   },
   "source": [
    "# **Question 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TenBkDJ5n95k"
   },
   "source": [
    "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
    "\n",
    "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
    "\n",
    "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
    "\n",
    "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
    "\n",
    "(4) Collect the top 100 tweets by using hashtag [\"#CovidVaccine\"](https://twitter.com/hashtag/CovidVaccine) from Twitter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuFPKhC0m1fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review\n",
      "0  Every once in a while a movie comes, that trul...\n",
      "1  This is a movie that only those who have felt ...\n",
      "2  Truly a masterpiece, The Best Hollywood film o...\n",
      "3  Joaquin Phoenix gives a tour de force performa...\n",
      "4  Most of the time movies are anticipated like t...\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "# (2)\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "URL = \"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\fatim\\Desktop\\chromedriver.exe\")\n",
    "driver.get(URL)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        driver.find_element_by_css_selector(\"button#load-more-trigger\").click()\n",
    "        WebDriverWait(driver,0.8).until(EC.invisibility_of_element_located((By.CSS_SELECTOR,\".ipl-load-more__load-indicator\")))\n",
    "    except Exception:break\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')        \n",
    "movie_containers = soup.find_all(class_ = 'lister-item-content')\n",
    "reviews = []\n",
    "\n",
    "for container in movie_containers:\n",
    "    text_review = container.find(class_ = 'content')\n",
    "    review = text_review.get_text().lstrip()\n",
    "    reviews.append(review)\n",
    "\n",
    "d = {'review': reviews}\n",
    "movie_reviews = pd.DataFrame(d)\n",
    "reviews_100 = pd.DataFrame(movie_reviews.head(100))\n",
    "print(reviews_100.head())\n",
    "\n",
    "reviews_100.to_csv(r\"C:\\Users\\fatim\\Desktop\\Output_CSV.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfpMRCrRwN6Z"
   },
   "source": [
    "# **Question 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dCQEbDawWCw"
   },
   "source": [
    "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
    "\n",
    "(1) Remove noise, such as special characters and punctuations.\n",
    "\n",
    "(2) Remove numbers.\n",
    "\n",
    "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
    "\n",
    "(4) Lowercase all texts\n",
    "\n",
    "(5) Stemming. \n",
    "\n",
    "(6) Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vATjQNTY8buA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Every once in a while a movie comes, that trul...</td>\n",
       "      <td>everi come truli make impact joaquin perform s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a movie that only those who have felt ...</td>\n",
       "      <td>thi felt alon isol truli relat you understand ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
       "      <td>truli masterpiec the best hollywood film one b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
       "      <td>joaquin phoenix give tour de forc perform fear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Most of the time movies are anticipated like t...</td>\n",
       "      <td>most time movi anticip like end fall short way...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Every once in a while a movie comes, that trul...   \n",
       "1  This is a movie that only those who have felt ...   \n",
       "2  Truly a masterpiece, The Best Hollywood film o...   \n",
       "3  Joaquin Phoenix gives a tour de force performa...   \n",
       "4  Most of the time movies are anticipated like t...   \n",
       "\n",
       "                                      cleaned_review  \n",
       "0  everi come truli make impact joaquin perform s...  \n",
       "1  thi felt alon isol truli relat you understand ...  \n",
       "2  truli masterpiec the best hollywood film one b...  \n",
       "3  joaquin phoenix give tour de forc perform fear...  \n",
       "4  most time movi anticip like end fall short way...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "reviews_100 = pd.DataFrame(movie_reviews[0:100])\n",
    "\n",
    "# (1) Removing Noise: Punctuations, Frequent words, Special/Rare words.\n",
    "\n",
    "# Punctuation removal.\n",
    "reviews_100['cleaned_review'] = reviews_100['review'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Frequent words removal.\n",
    "high_freq = pd.Series(' '.join(reviews_100['cleaned_review']).split()).value_counts()[:10]\n",
    "high_freq = list(high_freq.index)\n",
    "reviews_100['cleaned_review'] = reviews_100['cleaned_review'].apply(lambda x: \" \".join(x for x in x.split() if x not in high_freq))\n",
    "\n",
    "# Rare words removal.\n",
    "low_freq = pd.Series(' '.join(reviews_100['cleaned_review']).split()).value_counts()[-10:]\n",
    "low_freq = list(low_freq.index)\n",
    "reviews_100['cleaned_review'] = reviews_100['cleaned_review'].apply(lambda x: \" \".join(x for x in x.split() if x not in low_freq))\n",
    "\n",
    "# (2) Removing numbers.\n",
    "reviews_100['cleaned_review'] = reviews_100['cleaned_review'].str.replace('\\d+', '')\n",
    "\n",
    "# (3) Removing stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "reviews_100['cleaned_review'] = reviews_100['cleaned_review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# (4) Lower casing\n",
    "reviews_100['cleaned_review'] = reviews_100['cleaned_review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# (5) Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "reviews_100['cleaned_review'] = reviews_100['cleaned_review'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "# (6) Lemmatization\n",
    "from textblob import Word\n",
    "reviews_100['cleaned_review'] = reviews_100['cleaned_review'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "reviews_100.to_csv(r\"C:\\Users\\fatim\\Desktop\\Output_CSV.csv\", index=False)\n",
    "\n",
    "reviews_100.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5mmYIfN8eYV"
   },
   "source": [
    "# **Question 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsi2y4z88ngX"
   },
   "source": [
    "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
    "\n",
    "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
    "\n",
    "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
    "\n",
    "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQKnPjPDHJHr"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>POS_Tags</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Adjective_Count</th>\n",
       "      <th>Adverb_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Every once in a while a movie comes, that trul...</td>\n",
       "      <td>everi come truli make impact joaquin perform s...</td>\n",
       "      <td>[(everi, JJ), (come, VBP), (truli, NNS), (make...</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a movie that only those who have felt ...</td>\n",
       "      <td>thi felt alon isol truli relat you understand ...</td>\n",
       "      <td>[(thi, NN), (felt, VBD), (alon, JJ), (isol, NN...</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
       "      <td>truli masterpiec the best hollywood film one b...</td>\n",
       "      <td>[(truli, NN), (masterpiec, VBZ), (the, DT), (b...</td>\n",
       "      <td>40</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
       "      <td>joaquin phoenix give tour de forc perform fear...</td>\n",
       "      <td>[(joaquin, NN), (phoenix, NN), (give, VB), (to...</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Most of the time movies are anticipated like t...</td>\n",
       "      <td>most time movi anticip like end fall short way...</td>\n",
       "      <td>[(most, JJS), (time, NN), (movi, JJ), (anticip...</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Every once in a while a movie comes, that trul...   \n",
       "1  This is a movie that only those who have felt ...   \n",
       "2  Truly a masterpiece, The Best Hollywood film o...   \n",
       "3  Joaquin Phoenix gives a tour de force performa...   \n",
       "4  Most of the time movies are anticipated like t...   \n",
       "\n",
       "                                      cleaned_review  \\\n",
       "0  everi come truli make impact joaquin perform s...   \n",
       "1  thi felt alon isol truli relat you understand ...   \n",
       "2  truli masterpiec the best hollywood film one b...   \n",
       "3  joaquin phoenix give tour de forc perform fear...   \n",
       "4  most time movi anticip like end fall short way...   \n",
       "\n",
       "                                            POS_Tags  Noun_Count  Verb_Count  \\\n",
       "0  [(everi, JJ), (come, VBP), (truli, NNS), (make...          34           7   \n",
       "1  [(thi, NN), (felt, VBD), (alon, JJ), (isol, NN...          24          10   \n",
       "2  [(truli, NN), (masterpiec, VBZ), (the, DT), (b...          40          17   \n",
       "3  [(joaquin, NN), (phoenix, NN), (give, VB), (to...          31          13   \n",
       "4  [(most, JJS), (time, NN), (movi, JJ), (anticip...          29          11   \n",
       "\n",
       "   Adjective_Count  Adverb_Count  \n",
       "0               10             2  \n",
       "1                7             2  \n",
       "2               18             4  \n",
       "3                7             0  \n",
       "4               14             2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "# (1) Parts of Speech (POS) Tagging\n",
    "\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "\n",
    "reviews_100['POS_Tags'] = reviews_100['cleaned_review'].apply(lambda x: pos_tag(nltk.word_tokenize(x)))\n",
    "\n",
    "def NounCounter(x):\n",
    "    nouns = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"NN\"):\n",
    "            nouns.append(word)\n",
    "    return nouns\n",
    "\n",
    "reviews_100[\"Nouns\"] = reviews_100['POS_Tags'].apply(NounCounter)\n",
    "reviews_100[\"Noun_Count\"] = reviews_100[\"Nouns\"].str.len()\n",
    "\n",
    "def VerbCounter(x):\n",
    "    verbs = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"VB\"):\n",
    "            verbs.append(word)\n",
    "    return verbs\n",
    "\n",
    "reviews_100[\"Verbs\"] = reviews_100['POS_Tags'].apply(VerbCounter)\n",
    "reviews_100[\"Verb_Count\"] = reviews_100[\"Verbs\"].str.len()\n",
    "\n",
    "def AdjectiveCounter(x):\n",
    "    adjectives = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"JJ\"):\n",
    "            adjectives.append(word)\n",
    "    return adjectives\n",
    "\n",
    "reviews_100[\"Adjectives\"] = reviews_100['POS_Tags'].apply(AdjectiveCounter)\n",
    "reviews_100[\"Adjective_Count\"] = reviews_100[\"Adjectives\"].str.len()\n",
    "\n",
    "def AdverbCounter(x):\n",
    "    adverbs = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"RB\"):\n",
    "            adverbs.append(word)\n",
    "    return adverbs\n",
    "\n",
    "reviews_100[\"Adverbs\"] = reviews_100['POS_Tags'].apply(AdverbCounter)\n",
    "reviews_100[\"Adverb_Count\"] = reviews_100[\"Adverbs\"].str.len()\n",
    "\n",
    "reviews_100 = reviews_100.drop(['Nouns', 'Verbs', 'Adjectives', 'Adverbs'], axis = 1) \n",
    "\n",
    "reviews_100.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-4d9e426b6fc3>:11: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  con_parser = StanfordParser(path_to_jar=r\"C:\\Users\\fatim\\Desktop\\stanford-parser-4.0.0\\stanford-parser.jar\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[[(S\\n  (ADVP (RB everi))\\n  (VP\\n    (VB com...\n",
      "1    [[[(PRN\\n  (S\\n    (NP (NN thi))\\n    (VP\\n   ...\n",
      "2    [[[(PRN\\n  (S\\n    (NP (NNS truli))\\n    (VP\\n...\n",
      "3    [[[(S\\n  (VP\\n    (VB joaquin)\\n    (S (NP (NN...\n",
      "4    [[[(NP (JJS most) (NN time)), (NP\\n  (NP (NN m...\n",
      "Name: Constituency_Parsing_Tree, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# (2) Constituency Parsing and Dependency Parsing\n",
    "\n",
    "# Constituency Parsing\n",
    "\n",
    "import os\n",
    "java_path = r\"C:\\Program Files\\Java\\jre1.8.0_261\\bin\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "con_parser = StanfordParser(path_to_jar=r\"C:\\Users\\fatim\\Desktop\\stanford-parser-4.0.0\\stanford-parser.jar\",\n",
    "                            path_to_models_jar=r\"C:\\Users\\fatim\\Desktop\\stanford-parser-4.0.0\\stanford-parser-4.0.0-models.jar\")\n",
    "\n",
    "reviews_100['50_words_only'] = reviews_100['cleaned_review'].apply(lambda x: \" \".join(x.split()[0:50]))\n",
    "\n",
    "constituency_parsing_trees = []\n",
    "\n",
    "cleaned_reviews_list = reviews_100['50_words_only'].tolist()\n",
    "for x in cleaned_reviews_list:\n",
    "    con_parser_tree = list(con_parser.raw_parse(x))\n",
    "    constituency_parsing_trees.append(con_parser_tree)\n",
    "\n",
    "reviews_100['Constituency_Parsing_Tree'] = constituency_parsing_trees\n",
    "print(reviews_100['Constituency_Parsing_Tree'].head(5))\n",
    "\n",
    "# Print the Constituency Parsing Tree\n",
    "con = con_parser.raw_parse(reviews_100['50_words_only'][0])\n",
    "for line in con:\n",
    "    line.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-4237c87190be>:5: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  dep_parser = StanfordDependencyParser(path_to_jar=r\"C:\\Users\\fatim\\Desktop\\stanford-parser-4.0.0\\stanford-parser.jar\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [defaultdict(<function DependencyGraph.__init_...\n",
      "1    [defaultdict(<function DependencyGraph.__init_...\n",
      "2    [defaultdict(<function DependencyGraph.__init_...\n",
      "3    [defaultdict(<function DependencyGraph.__init_...\n",
      "4    [defaultdict(<function DependencyGraph.__init_...\n",
      "Name: Dependency_Parsing_Tree, dtype: object\n",
      "(review\n",
      "  (come everi (make truli (perform (joaquin impact))))\n",
      "  (watch\n",
      "    scenographi\n",
      "    brillianc\n",
      "    grotesqu\n",
      "    haunt\n",
      "    cringi\n",
      "    hard\n",
      "    watch\n",
      "    time\n",
      "    mesmer\n",
      "    wont\n",
      "    blink\n",
      "    eye\n",
      "    (funni tragic serious)\n",
      "    (timethi moment emot rollercoast sometim multipl emot poppingup))\n",
      "  (predict\n",
      "    (actionriddl (typic far))\n",
      "    (thrillerdrama superhero proper psycholog))\n",
      "  singl\n",
      "  (develop (charact best) (seen i ever))\n",
      "  found\n",
      "  help\n",
      "  wa)\n"
     ]
    }
   ],
   "source": [
    "# Dependency Parsing\n",
    "\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "dep_parser = StanfordDependencyParser(path_to_jar=r\"C:\\Users\\fatim\\Desktop\\stanford-parser-4.0.0\\stanford-parser.jar\",\n",
    "                            path_to_models_jar=r\"C:\\Users\\fatim\\Desktop\\stanford-parser-4.0.0\\stanford-parser-4.0.0-models.jar\")\n",
    "\n",
    "dependency_parsing_trees = []\n",
    "\n",
    "for x in cleaned_reviews_list:\n",
    "    dep_parser_tree = list(dep_parser.raw_parse(x))\n",
    "    dependency_parsing_trees.append(dep_parser_tree)\n",
    "\n",
    "reviews_100['Dependency_Parsing_Tree'] = dependency_parsing_trees\n",
    "print(reviews_100['Dependency_Parsing_Tree'].head(5))\n",
    "\n",
    "# Print the Dependency Parsing Tree\n",
    "dep = list(dep_parser.raw_parse(reviews_100['50_words_only'][0]))\n",
    "dep_tree = [parse.tree() for parse in dep][0]\n",
    "print(dep_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>Person_Names_Count</th>\n",
       "      <th>Organizations_Count</th>\n",
       "      <th>Locations_Count</th>\n",
       "      <th>Products_Count</th>\n",
       "      <th>Dates_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>everi come truli make impact joaquin perform s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thi felt alon isol truli relat you understand ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>truli masterpiec the best hollywood film one b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joaquin phoenix give tour de forc perform fear...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>most time movi anticip like end fall short way...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cleaned_review  Person_Names_Count  \\\n",
       "0  everi come truli make impact joaquin perform s...                   0   \n",
       "1  thi felt alon isol truli relat you understand ...                   0   \n",
       "2  truli masterpiec the best hollywood film one b...                   0   \n",
       "3  joaquin phoenix give tour de forc perform fear...                   0   \n",
       "4  most time movi anticip like end fall short way...                   0   \n",
       "\n",
       "   Organizations_Count  Locations_Count  Products_Count  Dates_Count  \n",
       "0                    0                0               0            0  \n",
       "1                    0                0               0            0  \n",
       "2                    0                0               0            0  \n",
       "3                    0                0               0            0  \n",
       "4                    0                0               0            1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3) Named Entity Recognition\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "st = StanfordNERTagger(r\"C:\\Users\\fatim\\Desktop\\stanford-ner-4.0.0\\classifiers\\english.muc.7class.distsim.crf.ser.gz\",\n",
    "                       r\"C:\\Users\\fatim\\Desktop\\stanford-ner-4.0.0\\stanford-ner.jar\")\n",
    "\n",
    "reviews_100['NER'] = reviews_100['cleaned_review'].apply(lambda x: st.tag(nltk.word_tokenize(x)))\n",
    "\n",
    "def PersonNameCounter(x):\n",
    "    person_names = []\n",
    "    for (word, description) in x:\n",
    "        if description.startswith(\"PERSON\"):\n",
    "            person_names.append(word)\n",
    "    return person_names\n",
    "\n",
    "reviews_100['Person_Names'] = reviews_100['NER'].apply(PersonNameCounter)\n",
    "reviews_100['Person_Names_Count'] = reviews_100['Person_Names'].str.len()\n",
    "\n",
    "def OrganizationCounter(x):\n",
    "    organizations = []\n",
    "    for (word, description) in x:\n",
    "        if description.startswith(\"ORGANIZATION\"):\n",
    "            organizations.append(word)\n",
    "    return organizations\n",
    "\n",
    "reviews_100['Organizations'] = reviews_100['NER'].apply(OrganizationCounter)\n",
    "reviews_100['Organizations_Count'] = reviews_100['Organizations'].str.len()\n",
    "\n",
    "def LocationCounter(x):\n",
    "    locations = []\n",
    "    for (word, description) in x:\n",
    "        if description.startswith(\"LOCATION\"):\n",
    "            locations.append(word)\n",
    "    return locations\n",
    "\n",
    "reviews_100['Locations'] = reviews_100['NER'].apply(LocationCounter)\n",
    "reviews_100['Locations_Count'] = reviews_100['Locations'].str.len()\n",
    "\n",
    "def ProductCounter(x):\n",
    "    products = []\n",
    "    for (word, description) in x:\n",
    "        if description.startswith(\"PRODUCT\"):\n",
    "            products.append(word)\n",
    "    return products\n",
    "\n",
    "reviews_100['Products'] = reviews_100['NER'].apply(ProductCounter)\n",
    "reviews_100['Products_Count'] = reviews_100['Products'].str.len()\n",
    "\n",
    "def DateCounter(x):\n",
    "    dates = []\n",
    "    for (word, description) in x:\n",
    "        if description.startswith(\"DATE\"):\n",
    "            dates.append(word)\n",
    "    return dates\n",
    "\n",
    "reviews_100['Dates'] = reviews_100['NER'].apply(DateCounter)\n",
    "reviews_100['Dates_Count'] = reviews_100['Dates'].str.len()\n",
    "\n",
    "reviews_100 = reviews_100.drop(['Person_Names', 'Organizations', 'Locations', 'Products', 'Dates'], axis = 1) \n",
    "reviews_100[['cleaned_review', 'Person_Names_Count', 'Organizations_Count', 'Locations_Count', 'Products_Count', 'Dates_Count']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWOtvT2rHNWy"
   },
   "source": [
    "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWrite your explanations of the constituency parsing tree and dependency parsing tree here\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Write your explanations of the constituency parsing tree and dependency parsing tree here\n",
    "\n",
    "Constituency Parsing Tree:\n",
    "\n",
    "I have created the Constituency Parsing Tree for the first 50 words in each review. This is because the parser uses a lot of memory. \n",
    "\n",
    "I have printed out the Constituency Parsing Tree for the first review in our reviews_100 table. \n",
    "The Tree prints out in a new NLTK window. This tree divides a sentence into its constituents. \n",
    "Words are the constituents of a sentence and make up the terminal nodes of a parsing tree. \n",
    "\n",
    "At the first level below the ROOT, the review is fragmented into 4S, 1 FRAG, 1 Adjective Phrase (ADJP) and 1 Noun Phrase (NP).\n",
    "\n",
    "1. The first S is then split into an Adverb Phrase (ADVP) and a Verb Phrase (VP).\n",
    "   The ADVP has 1 Adverb (RB) 'everi'.\n",
    "   The VP has 1 Verb (VB) 'come' and 1 S.\n",
    "       The S has 1 NP and 1 VP.\n",
    "       The NP has 1 Plural Noun (NNS) 'truli'\n",
    "       The VP has 1 VB 'make' and 1 S.\n",
    "           The S has 1 NP and 1 VP.\n",
    "           The NP has 2 Nouns (NN) 'imapct' and 'joaquin'.\n",
    "           The VP has 1 VB 'perform'\n",
    "\n",
    "2. The FRAG is split into 3 NPs.\n",
    "   The first NP has 13 NNs.\n",
    "   The second NP has 2 Adjectives (JJ) and NNS.\n",
    "   The third NP has 8 NNs.\n",
    "   \n",
    "3. The second S is split into 1 NP and 1 VP.\n",
    "   The NP has 1 ADJP and 1 NN 'actionriddl'.\n",
    "       The ADJP has 1 RB 'far' and 1 JJ 'typic'.\n",
    "    The VP has 1 VBP 'predict' and 1 NP.\n",
    "       The NP has 2 JJs, 1 NN and 1 NNS.\n",
    "       \n",
    "4. The ADJP has 1 JJ 'singl'.\n",
    "\n",
    "5. The third S has 1 NP and 1 VP.\n",
    "   The NP has 1 JJS 'best' and 1 NN 'charact'\n",
    "   The VP has 1 VBP 'develop' and 1 S.\n",
    "       The S has 1 NP, 1 ADVP and 1 VP.\n",
    "       The NP has 1 PRP 'i'.\n",
    "       The ADVP has 1 RB 'ever'.\n",
    "       The P has 1 VBN 'seen'.\n",
    "       \n",
    "6. The fourth S has 1 VP.\n",
    "   The VP has 1 VBN 'found'.\n",
    "   \n",
    "7. The NP has 3 NNs. \n",
    "\n",
    "******************************************************************************************************************************\n",
    "\n",
    "Dependency Parsing Tree:\n",
    "\n",
    "I have created the Dependency Parsing Tree for the first 50 words in each review. This is because the parser uses a lot of memory. \n",
    "\n",
    "I have printed out the Dependency Parsing Tree for the first review in our reviews_100 table. \n",
    "The Dependency Parsing Tree shows the syntax of a sentence by highlighting dependencies between the words.\n",
    "\n",
    "The roots of the tree are 'come', 'watch', 'predict', 'singl', 'develop', 'found', 'help' and 'wa'.\n",
    "\n",
    "1.'come' node has 'everi' and 'make' depending on it.\n",
    "    Furthermore, 'make' has 'truli' and 'perform' depending on it.\n",
    "        Then 'perform' has 'joaquin' depending on it.\n",
    "            And, lastly, 'joaquin' has 'impact' depending on it.\n",
    "            \n",
    "2.'watch' node has 14 words depending on it: 'scenographi', 'brillianc', 'grotesqu', ..., 'funni' and 'timethi'.\n",
    "    12 of the words do not have any further words depending on them.\n",
    "    'funni' has 'tragic' and 'serious' depending on it.\n",
    "    'timethi' has 7 words depending on it: 'moment', 'emot', 'rollercoast', etc.\n",
    "    \n",
    "3.'predict' node has 'actionriddl' and 'thrillerdrama' depending on it. \n",
    "    Furthermore, 'actionriddl' has 'typic' depending on it.\n",
    "        And, lastly, 'typic' has 'far' depending on it.\n",
    "    Furthermore, 'thrillerdrama' has 'superhero', 'proper' and 'psycholog' depending on it.\n",
    "\n",
    "4.'develop' has 'charact' and 'seen' depending on it.\n",
    "    Furthermore, 'charact' has 'best' depending on it.\n",
    "    And, 'seen' has 'i' and 'ever' depending on it.\n",
    "    \n",
    "5. 'singl', 'found', 'help' and 'wa' are root words with no other words depending on them. \n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNljc+o/lciN8B4S0X1+1d9",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "INFO5731_Assignment_Two.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
